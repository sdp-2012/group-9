package sdp.server.vision;

import java.awt.image.BufferedImage;

import sdp.server.BallState;
import sdp.server.RobotState;
import sdp.server.ScreenProjection;
import sdp.server.math.MathUtils;
import sdp.server.math.Vector2;

import com.googlecode.javacv.cpp.opencv_core.CvContour;
import com.googlecode.javacpp.Loader;

import static com.googlecode.javacv.cpp.opencv_core.*;
import static com.googlecode.javacv.cpp.opencv_imgproc.*;


public class Vision {
	// FIXME: This should be set to "", but I can't figure out where Eclipse puts its working directory
	private static String dirRoot    = ".";
	private static String dataFolder = dirRoot + "/Data";
	
	private static final int CROPPED_ONE_CHANNEL_IMAGES = 16;
	private static final int CROPPED_THREE_CHANNEL_IMAGES = 4;
	private static final int UNCROPPED_THREE_CHANNEL_IMAGES = 4;
	private static final int UNCROPPED_ONE_CHANNEL_IMAGES = 8;
	
	private IplImage   raw, undistorted, cropped;
	private IplImage[] cropped1, cropped3;
	private IplImage[] uncropped3, uncropped1;
	private CvMemStorage storage = CvMemStorage.create();
	private IplImage mapx, mapy;
	private IplImage  debugImage;
	private ContourModel[]   contourModels;
	private ScreenProjection screen;
	
	private int frameId     = 0;
	private int showChannel = -1;
	
	private EntityLearner[] learners = {
			new EntityLearner( Entity.BALL ),
			new EntityLearner( Entity.YELLOW_ROBOT ),
			new EntityLearner( Entity.BLUE_ROBOT ),
			new EntityLearner( Entity.BLACK_CIRCLE ),
			new EntityLearner( Entity.PITCH )
	};
	
	private int[][][] entProps = { { {   7,  8 }, {  24,  9 }, {  79, 24 }, { 0, 0 }, { 1,  2 } },   // HUE
                                   { { 166, 88 }, { 166, 88 }, { 166, 88 }, { 0, 0 }, { 3,  4 } },   // SAT
                                   { { 187, 67 }, { 187, 67 }, { 187, 67 }, { 0, 0 }, { 5,  6 } } }; // VAL
	
	private CvScalar[] entDbgCols = { cvScalar(0,0,255,0), cvScalar(0,255,255,0), cvScalar(255,0,0,0) };
	private int[] suppressAngle = { 0, 0 };
	
	//private CvRect cropRect = cvRect(30, 105, 640, 360);
	//private CvRect cropRect = cvRect(0,0,720,576);
	private CvRect cropRect = cvRect(0, 90, 720, 400);
	
	private CvRect trainRect   = null;
	private Entity trainEntity = Entity.BALL;
	private int    trainUntil  = -1; 
	private double kSigma      = 1.0;
	
	static int     BALL_CHANNEL          = 0;
	static int     YELLOW_CHANNEL        = 1;
	static int     BLUE_CHANNEL          = 2;
	static int     YELLOW_BLACK_CHANNEL  = 3;
	static int     BLUE_BLACK_CHANNEL    = 4;
	static int     PITCH_CHANNEL         = 5;
		
	public enum Mode {
		RAW,
		CHANNELS,
		PITCH
	}
	
	public enum Value {
		MEAN,
		RANGE
	}
	
	public enum Property {
		HUE, SATURATION, VALUE
	}
	
	public enum Entity {
		BALL, YELLOW_ROBOT, BLUE_ROBOT, BLACK_CIRCLE, PITCH
	}
	
	public Vision( ScreenProjection screenProjection ) {
		screen = screenProjection;
		updateProjection();
		
		contourModels = new ContourModel[ 3 ];
		for( int i = 0 ; i < 3 ; ++ i )
			contourModels[ i ] = new ContourModel();
	}
	
	public void processImage( BufferedImage frame, RobotState[] robotStates, BallState ball, double timeStep ) {
		if( frameId == 0 ) {
			allocUncroppedImages( frame );
			allocCroppedImages();
			loadCalibrationInfo();
			loadProperties();
		} else {
			raw.copyFrom( frame );
		}
		
		undistort();
		applyCrop();
		
		// FIXME: Move training in a separate method.
		if( isTraining() ) {
			learners[ trainEntity.ordinal() ].learn( trainRect );
		} else if( isJustDoneTraining() ) {
			System.out.println("Vision: Learning result:");
			for( Property prop : Property.values() ) {
				System.out.println("Vision: " + trainEntity.toString() + " " + prop.toString() + " = " + getPropertyMin(trainEntity, prop) + "-" + getPropertyMax(trainEntity, prop));
			}
			resetTrainRect();
		}
		
		debugImage = detectEntities( robotStates, ball, timeStep );
		frameId ++;
	}
	
	public void updateProjection() {
		screen.setScreenSize( new Vector2( cropRect.width(), cropRect.height() ) );
		screen.setScale( ( 710 * 0.00344 ) / cropRect.width() );
	}
	
	public void resetContourModels() {
		for( int i = 0 ; i < 3 ; ++ i )
			contourModels[ i ].reset();
	}
	
	public void resetColourModels() {
		
	}
	
	public IplImage getDebugImage() {
		return debugImage;
	}
	
	public boolean isShowingChannel() {
		// TODO: Change channel showing into a more robust debug level system.
		return showChannel != -1;
	}
	
	public void showEntityChannel( Entity e ) {
		showChannel = e.ordinal(); 
	}
	
	public void showNormal() {
		showChannel = -1;
	}
	
	// TODO: These methods, while general, are pretty cumbersome to use. Would be nice to have
	// convenience methods that wrap them (e.g. setMeanHue( entity, newMeanValue ) etc. ).
	public void setProperty( Entity entity, Property prop, Value valueType, int value ) {
		entProps[ prop.ordinal() ][ entity.ordinal()  ][ valueType.ordinal() ] = value;
	}
	
	public int getProperty( Entity type, Property prop, Value valueType ) {
		return entProps[ prop.ordinal() ][ type.ordinal() ][ valueType.ordinal() ];
	}
	
	public int getPropertyMean( Entity type, Property prop ) {
		return getProperty( type, prop, Value.MEAN );
	}
	
	public int getPropertyRange( Entity type, Property prop ) {
		return getProperty( type, prop, Value.RANGE );
	}
	
	public int getPropertyMin( Entity type, Property prop ) {
		return clamp255( getProperty( type, prop, Value.MEAN) -
		                 getProperty( type, prop, Value.RANGE)
		);
	}
	
	public int getPropertyMax( Entity type, Property prop ) {
		return clamp255( getProperty( type, prop, Value.MEAN) +
		                 getProperty( type, prop, Value.RANGE)
		);
	}
	
	public void setCropLeft( int newValue ) {
		cropRect.x( newValue );
	}
	
	public void setCropTop( int newValue ) {
		cropRect.y( newValue );
	}
	
	public void setCropWidth( int newValue ) {
		cropRect.width( newValue );
	}
	
	public void setCropHeight( int newValue ) {
		cropRect.height( newValue );
	}
	
	public int getCropLeft() {
		return cropRect.x();
	}
	
	public int getCropTop() {
		return cropRect.y();
	}
	
	public int getCropWidth() {
		return cropRect.width();
	}
	
	public int getCropHeight() {
		return cropRect.height();
	}
	
	public double getKSigma() {
		return kSigma;
	}
	
	public void setKSigma( double newValue ) {
		kSigma = newValue;
		for( EntityLearner el : learners )
			el.refresh();
	}
	
	public void setTrainRect( int x1, int y1, int x2, int y2 ) {
		if(!isTraining()) {
			int aux;
			if( x1 > x2 ) { aux = x1; x1 = x2; x2 = aux; }
			if( y1 > y2 ) { aux = y1; y1 = y2; y2 = aux; }
			
			trainRect = cvRect(x1,y1,x2-x1,y2-y1);
		}
	}
	
	public void resetTrainRect() {
		if(!isTraining())
			trainRect = null;
	}
	
	public void train( Entity e ) {
		if( trainRect == null ) {
			System.out.println("Vision: No training rectangle.");
		} else if( isTraining() ) {
			System.out.println("Vision: Already training...");
		} else {
			System.out.println("Vision: Started training...");
			trainUntil  = frameId + 50;
			trainEntity = e;
		}
	}
	
	public void saveProperties() {
		saveProperties("entity_props.xml");
	}
	
	public void saveProperties( String filename ) {
		CvMat data = new CvMat( cvLoad(dataFolder + "/" + filename) );
		
		if( !data.isNull() ) {
			System.out.println("Vision: Property file already exists, backing up...");
			cvSave(dataFolder + "/b4save_" + filename, data);
		}
		
		int m = Property.values().length;
		int n = Entity.values().length;
		int p = Value.values().length;
		
		data = CvMat.create( m*n*p, 1 );
		
		int s = 0;
		for( int i = 0 ; i < m ; ++ i )
			for( int j = 0 ; j < n ; ++ j )
				for( int k = 0 ; k < p ; ++ k ) {
					data.put(s ++, entProps[i][j][k] );
				
				}
		
		cvSave(dataFolder + "/" + filename, data);
		System.out.println("Vision: Properties saved.");
	}
	
	public void loadProperties() {
		loadProperties("entity_props.xml");
	}
	
	public void loadProperties( String filename ) {
		System.out.println("Vision: Saving current properties...");
		saveProperties( "b4load_entity_props.xml" );
		System.out.println("Vision: Loading new properties...");
		CvMat data = new CvMat( cvLoad(dataFolder + "/" + filename) );
		
		int m = Property.values().length;
		int n = Entity.values().length;
		int p = Value.values().length;
		
		if( data.isNull() ) {
			System.err.println("Vision: Could not open file.");
		} else if( data.rows() != m*n*p || data.cols() != 1 ) {
			System.err.println("Vision: Invalid dimensions in file.");
		} else {
			int s = 0;
			for( int i = 0 ; i < m ; ++ i )
				for( int j = 0 ; j < n ; ++ j )
					for( int k = 0 ; k < p ; ++ k )
						entProps[ i ][ j ][ k ] = (int)data.get( s ++ );
		}
	}
	
	public BufferedImage autoCrop() {
		IplImage blur  = uncropped3[ 0 ], hsv = uncropped3[ 1 ];
		IplImage hue   = uncropped1[ 0 ], sat = uncropped1[ 1 ], val = uncropped1[ 2 ];
		IplImage tmp1  = uncropped1[ 3 ], tmp2 = uncropped1[ 4 ];
		IplImage pitch = uncropped1[ 5 ];
		
		// Add a linear blur filter to get rid of a bit of grain (we don't care about fine detail here)
		cvSmooth( raw, blur, CV_BLUR, 5 );
		
		// Convert the blurred image to HSV, save the channels in hue, sat and val
		cvCvtColor( blur, hsv, CV_BGR2HSV );
		cvSplit( hsv, hue, sat, val, null );
		
		// Select pixels with minHue[PITCH] < hue < maxHue[PITCH], save mask in pitch
		cvThreshold( hue, tmp1, getPropertyMin( Entity.PITCH, Property.HUE ), 255, CV_THRESH_BINARY );
		cvThreshold( hue, tmp2, getPropertyMax( Entity.PITCH, Property.HUE ), 255, CV_THRESH_BINARY_INV );
		cvAnd( tmp1, tmp2, pitch, null );
		
		// Further select non-dark pixels (val > minValue / 2) to get rid of black boundaries of the pitch.
		cvThreshold( val, tmp1, getPropertyMin( Entity.PITCH, Property.VALUE ) * .5, 255, CV_THRESH_BINARY );
		cvAnd( pitch, tmp1, pitch, null );
		
		// Get the contours in the pitch mask
		CvSeq seq = new CvSeq(null);
		cvFindContours(pitch, storage, seq,Loader.sizeof(CvContour.class), CV_RETR_LIST, CV_CHAIN_APPROX_SIMPLE);
		
		// If there are any contours, find the one with the largest bounding box in pitch
		if( !seq.isNull() ) {
			CvRect maxRect = null;
			for( CvSeq s = seq ; s != null && !s.isNull() ; s = s.h_next() ) {
				CvRect r = cvBoundingRect(s, 0);
				if( maxRect == null || r.width()*r.height() > maxRect.width()*maxRect.height() )
					maxRect = r;
			}
			
			// Update the crop rectangle to the max rectangle
			System.out.println("Vision: Pitch: Rect = " + maxRect.x() + ", " + maxRect.y() + " ; " + maxRect.width() + ", " + maxRect.height() );
			cropRect = maxRect;
			
			// Realloc images to the size of the crop rectangle
			allocCroppedImages();
			
			// Ensure the projection uses the new screen size
			updateProjection();
		}
		
		return raw.getBufferedImage();
	}
	
	private IplImage detectEntities( RobotState[] robots, BallState ball, double timeStep ) {
		IplImage   ret = cropped3[ 0 ], temp3 = cropped3[ 1 ], temp1 = cropped1[ 0 ];
		IplImage[] hsv = { cropped1[ 2 ], cropped1[ 3 ], cropped1[ 4 ] };
		IplImage[] ech  = { cropped1[ 5 ], cropped1[ 6 ], cropped1[ 7 ], cropped1[ 8 ], cropped1[ 9 ] };
		IplImage   debugImage = isShowingChannel() ? ret : null;
		CvRect   cropNothing = cvRect(0, 0, cropRect.width(), cropRect.height());
		CvRect[] roi         = { cropNothing, cropNothing, cropNothing };
		if( frameId % 10 != 0 ) {
			int w = 128;
			int h = 128;
			Vector2[] pos = { ball.getPosition(), robots[ 0 ].getPosition(), robots[ 1 ].getPosition() };
			
			for( int i = 0 ; i < 3 ; ++ i) {
				pos[ i ] = screen.projectPosition( pos[ i ] );
				roi[ i ] = cvRect( pos[ i ].getIntX() - w / 2, pos[ i ].getIntY() - h / 2, w, h );
				clampCropRect( roi[ i ], ret );
			}
		}
		
		// We will be drawing debug info on the image, so we want to leave
		// cropped unchanged.
		cvCopy( cropped, ret );
		computeEntityChannels( hsv, ech, roi, temp1, temp3, debugImage );
		
		// Detect the robots and ball
		for( int i = 0 ; i < 2 ; ++ i )
			detectRobot( ech[ i + 1 ], roi, temp1, robots[ i ], timeStep, debugImage );
		
		detectBall( ech[ 0 ], roi, temp1, ball, timeStep, ret );
		drawDetectedEntites( ret, robots, ball );
		
		if( trainRect != null )
			cvRectangleR(ret, trainRect, cvScalar(255,255,0,0), 1, 8, 0);
		
		return ret;
	}
	
	private void computeEntityChannels( IplImage[] hsv, IplImage[] channels, CvRect[] rois, IplImage temp1, IplImage temp3, IplImage debugImage ) {
		// Always compute the ball & robots channel. If we're debugging showChannel, also compute that.
		int nChannels = Math.max( 2, showChannel ) + 1;
		
		// Separate image into hue, sat and value
		cvCvtColor( cropped, temp3, CV_BGR2HSV );
		cvSplit( temp3, hsv[0], hsv[1], hsv[2], null );
		
		// Each entity's channel is a binary image with white where the pixels in the original
		// image lie within the HSV limits of the entity.
		for( int c = 0 ; c < nChannels; ++ c ) {
			Entity   e  = Entity.values()[ c ];
			IplImage ch = channels[ c ];
			
			// For each pixel property (hue, saturation or value), find the pixels that lie
			// between the entity limits for that property.
			for( int i = 0; i < 3 ; ++ i ) {
				cvSetImageROI( hsv[ i ], rois[ c ] );
				cvSetImageROI( ch, rois[ c ] );
				cvSetImageROI( temp1, rois[ c ] );
				Property p    = Property.values()[ i ];
				IplImage hsvi = hsv[ i ];
				if( i == 0 ) {
					cvThreshold( hsvi, ch, getPropertyMin( e, p ), 255.0, CV_THRESH_BINARY );
				} else {
					cvThreshold( hsvi, temp1, getPropertyMin( e, p ), 255.0, CV_THRESH_BINARY );
					cvAnd( ch, temp1, ch, null );
				}
				
				cvThreshold( hsvi, temp1, getPropertyMax( e, p ), 255.0, CV_THRESH_BINARY_INV );
				cvAnd( ch, temp1, ch, null );
				cvResetImageROI( hsv[ i ] );
				cvResetImageROI( ch );
				cvResetImageROI( temp1 );
			}
			
			// Apply median filter to get rid of salt and pepper noise
			//cvSmooth( ch, ch, CV_MEDIAN, 3);
		}
		
		// Superimpose the three entity channels over the debugImage, if required.
		if( debugImage != null ) {
			if( showChannel < 3 ) {
				// If we're debugging one of the object channels, then we show all of them.
				cvSet( temp1, cvScalarAll(0) );
				for( int c = 0 ; c < 3 ; ++ c )
					cvOr( temp1, channels[ c ], temp1, null);
				
				cvMerge( channels[2], channels[1], channels[0], null, temp3 );
				cvAndS( debugImage, cvScalarAll(0), debugImage, temp1 );
				cvOr( debugImage, temp3, debugImage, null );
				
				for( int i = 0 ; i < 5 ; ++ i )
					cvRectangleR( debugImage, rois[ i ], cvScalar(255,255,255,0), 1, 8, 0);
			} else {
				// Otherwise, if we're debugging the pitch or black circle channel, show only that.
				cvMerge(
					channels[ showChannel ],
					channels[ showChannel ],
					channels[ showChannel ],
					null, debugImage
				);
			}
		}
	}
	
	private void detectRobot( IplImage channel, CvRect[] roi, IplImage temp, RobotState state, double timeStep, IplImage debugImage ) {
		int          index    = state.getTeam().ordinal() + 1;
		ContourModel model    = contourModels[ index ];
		cvSetImageROI( channel, roi[ index ] );
		cvSetImageROI( temp,    roi[ index ] );
		CvSeq        contours = getContours( channel, temp );
		cvResetImageROI( channel );
		cvResetImageROI( temp );
		CvSeq        contour  = model.findContour( contours, state.getLinearVelocity() );
		
		Vector2 offsetContours = new Vector2( roi[index].x(), roi[index].y());
		
		if( contour != null ) {
			// - Get the centroid of the whole contour.
			Vector2 newPosition = model.getCentroid();
			double  newAngle    = computeRobotOrientation(contour, offsetContours, newPosition, debugImage);
			
			updateRobot( state, newPosition.plus( offsetContours  ), newAngle, timeStep, debugImage );
		}
	}
	
	private void detectBall( IplImage channel, CvRect[] roi, IplImage temp, BallState state, double timeStep, IplImage debugImage ) {
		ContourModel model    = contourModels[ 0 ];
		cvSetImageROI( channel, roi[ 0 ] );
		cvSetImageROI( temp, roi[ 0 ] );
		CvSeq        contours = getContours( channel, temp );
		cvResetImageROI( channel );
		cvResetImageROI( temp );
		CvSeq        contour  = model.findContour( contours, state.getLinearVelocity() );
		
		if( contour != null && !contour.isNull() ) {
			Vector2 ballPos = model.getCentroid().plus( new Vector2( roi[0].x(), roi[0].y()) );
			if( ballPos.isNaN() )
				System.out.println("Vision: NaN contour centroid.");
			else if( cvContourArea(contour, CV_WHOLE_SEQ, 0) > 9 ) {
				if( !ballPos.isNaN() )
					state.update( screen.unprojectPosition( ballPos ), timeStep );
			}
		}	
	}
	
	private void drawDetectedEntites( IplImage image, RobotState[] robots, BallState ball ) {
		// Draw robots
		for( RobotState robot : robots ) {
			Vector2 p1 = screen.projectPosition( robot.getPosition() );
			Vector2 p2 = p1.plus( Vector2.fromAngle( -robot.getRotation() ).times( 25.0 ) );
			CvPoint cp1 = p1.getCvPoint(), cp2 = p2.getCvPoint();
			CvScalar robotColour = entDbgCols[ robot.getTeam().ordinal() + 1 ];
			cvLine( image, cp1, cp2, cvScalarAll(230), 3, 8, 0 );
			cvLine( image, cp1, cp2, robotColour, 2, 8, 0 );
		}
		
		// Draw ball
		drawCenteredRectangle(
				image,
				screen.projectPosition( ball.getPosition() ),
				10,  entDbgCols[ Entity.BALL.ordinal() ], 1
		);
		
		{
			Vector2 p1 = ball.getPosition();
			Vector2 p2 = p1.plus( ball.getLinearVelocity().times(1.0/12.0) );
			
			p1 = screen.projectPosition( p1 ); p2 = screen.projectPosition( p2 );
			cvLine( image, p1.getCvPoint(), p2.getCvPoint(), cvScalarAll(255), 1, 8, 0 );
		}
	}
	
	private void updateRobot( RobotState robot, Vector2 newPosition, double newAngle, double timeStep, IplImage debugImage ) {
		int robotIdx = robot.getTeam().ordinal();
		int suppressFirstFrames = 5;
		int suppressAngleFrames = 25;
		
		newPosition = screen.unprojectPosition( newPosition );
		
		// - Smooth the robot position if not in the first few frames where things are fuzzy.
		if( frameId >= suppressFirstFrames && !robot.getPosition().isNaN() )
			newPosition = newPosition.times(0.9).plus( robot.getPosition().times(0.1) );
		
		// - Smooth and filter the robot rotation.
		double oldAngle = robot.getRotation();
		
		if( frameId < suppressFirstFrames || suppressAngle[ robotIdx ] > suppressAngleFrames ) {
			// If we're in the first few frames or if we've suppressed an angle change for too
			// long leave the newAngle unchanged. 
			suppressAngle[ robotIdx ] = 0; 
		} else if( Math.abs( MathUtils.angleDiff( newAngle, oldAngle + robot.getAngularVelocity() ) ) <= Math.PI/2 ) {
			// If the angle change is reasonable, don't suppress the angle, and smooth it
			// with the old one.
			//newAngle = Control.capAngle( newAngle * 0.9 + oldAngle * 0.1 );
			suppressAngle[ robotIdx ] = Math.max(0, suppressAngle[ robotIdx ] - 1 );
		} else {
			// If the angle change is too big, suppress the angle change
			newAngle = oldAngle;
			suppressAngle[ robotIdx ] ++;
			if( debugImage != null ) {
				Vector2 otherPoint = screen.projectPosition( newPosition ).plus( Vector2.fromAngle( newAngle ).times( 50.0 ) );
				cvLine( debugImage, screen.projectPosition( newPosition ).getCvPoint(), otherPoint.getCvPoint(), cvScalar(0,0,255,0), 1, 8, 0 );
			}
		}
		
		if( !newPosition.isNaN() && !Double.isNaN( newAngle ) )
			robot.update( newPosition, newAngle, timeStep );
	}

	private double computeRobotOrientation( CvSeq robotContour, Vector2 contourOffset, Vector2 robotCentroid, IplImage debugImage ) {
		// - Find the two largest convexity defects.
		CvSeq hull   = cvConvexHull2( robotContour, storage, CV_CLOCKWISE, 0 );
		CvSeq defect = cvConvexityDefects( robotContour, hull, storage );
		
		// We need to have at least two defects.
		if( defect.total() < 2 )
			return Double.NaN;
		
		// Find the two defects of largest depth
		CvConvexityDefect[] bigDefects = { null, null };
		for( int i = 0 ; i < defect.total() ; ++ i ) {
			CvConvexityDefect d = new CvConvexityDefect( cvGetSeqElem( defect, i ) );
			if( bigDefects[ 0 ] == null || d.depth() > bigDefects[ 0 ].depth() ) {
				bigDefects[ 1 ] = bigDefects[ 0 ];
				bigDefects[ 0 ] = d;
			} else if( bigDefects[ 1 ] == null || d.depth() > bigDefects[ 1 ].depth() ) {
				bigDefects[ 1 ] = d;
			}
		}
		
		// - Find the line between the two defects - we'll cut the contour along it. 
		CvPoint[] cutLine = { bigDefects[ 0 ].depth_point(), bigDefects[ 1 ].depth_point() };
		int[] cutIdx = { -1, -1 };
		for( int i = 0 ; i < robotContour.total() ; ++ i ) {
			CvPoint point = new CvPoint( cvGetSeqElem( robotContour, i ) );
			if( point.x() == cutLine[ 0 ].x() && point.y() == cutLine[ 0 ].y() ) {
				cutIdx[ 0 ] = i;
				if( cutIdx[ 1 ] != -1 )
					break;
			}
			if( point.x() == cutLine[ 1 ].x() && point.y() == cutLine[ 1 ].y() ) {
				cutIdx[ 1 ] = i;
				if( cutIdx[ 0 ] != -1 )
					break;
			}
		}
		
		// Ensure the indices are in ascending order
		if( cutIdx[ 0 ] > cutIdx[ 1 ] ) {
			int aux = cutIdx[ 0 ];
			cutIdx[ 0 ] = cutIdx[ 1 ];
			cutIdx[ 1 ] = aux;
		}
		
		// - Cut up the contour into two pieces, along cutLine
		CvSeq[] contours = { null, null };
		contours[ 0 ] = cvSeqSlice( robotContour, cvSlice( cutIdx[0], cutIdx[1] + 1), storage, 0 );
		contours[ 1 ] = cvSeqSlice( robotContour, cvSlice( cutIdx[1] - robotContour.total(), cutIdx[0] + 1), storage, 0 );
		
		// - Get the direction line.
		// The line is defined by the centroids of the two contour slices
		Vector2[] centroids = { computeCentroid( contours[ 0 ] ), computeCentroid( contours[ 1 ] ) };				
		Vector2   direction = centroids[ 1 ].minus(centroids[ 0 ]).computeUnit();
		
		// To figure out which way is forwards along direction, we compute the intersection of the line
		// with both contour slices. The direction will be from the intersection point closer to 
		// the centroid towards the one further away.
		Vector2[] intersects = { null, null };
		for( int i = 0 ; i < 2 ; ++ i ) {
			CvSeq c = contours[ i ];
			for( int j = 1 ; j < c.total() ; ++ j ) {
				Vector2 p1 = new Vector2( new CvPoint(cvGetSeqElem(c,j-1)) );
				Vector2 p2 = new Vector2( new CvPoint(cvGetSeqElem(c,j)) );
				Vector2 v  = p2.minus(p1);
				double d = lineIntersection( p1, v, robotCentroid, direction );
				double pix = 1.0 / v.computeNorm();
				if( d >= -pix && d <= 1.0 + pix ) {
					intersects[ i ] = p1.plus( v.times(d) );
					break;
				}
			}
		}
		
		// If, for whatever reason, we didin't find both the intersections
		// we use the centroids to figure out which way is forwards.
		boolean goodIntersects = intersects[0] != null && intersects[1] != null;
		if( !goodIntersects ) {
			intersects[ 0 ] = centroids[ 0 ];
			intersects[ 1 ] = centroids[ 1 ];
		}
			
		// If the intersection point of slice 1 is closer to the centroid, we need to reverse the direction.
		if( intersects[0].minus( robotCentroid ).computeSquaredNorm() > intersects[1].minus(robotCentroid).computeSquaredNorm() )
			direction  = direction.times( -1.0 );
		
		if( debugImage != null ) {			
			CvScalar red = cvScalar( 0, 0, 255, 0 );
			CvScalar mag = cvScalar( 255, 0, 255, 0 );
			CvScalar cyn = cvScalar( 255, 255, 0, 0 );
			CvScalar whi = cvScalarAll(255);
			cvLine( debugImage, contourOffset.plus( new Vector2( cutLine[0] ) ).getCvPoint(), contourOffset.plus( new Vector2( cutLine[1] ) ).getCvPoint(), red, 1, 8, 0 );
			
			drawCenteredRectangle( debugImage, centroids[0].plus(contourOffset), 3, mag, 1);
			drawCenteredRectangle( debugImage, centroids[1].plus(contourOffset), 3, mag, 1);
			drawCenteredRectangle( debugImage, robotCentroid.plus(contourOffset), 5, whi, 1);
			if( goodIntersects ) {
				drawCenteredRectangle( debugImage, intersects[0].plus(contourOffset), 4, cyn, 1);
				drawCenteredRectangle( debugImage, intersects[1].plus(contourOffset), 4, cyn, 1);
			} else {
				drawCenteredRectangle( debugImage, intersects[0].plus(contourOffset), 4, cyn, 1);
				drawCenteredRectangle( debugImage, intersects[1].plus(contourOffset), 4, cyn, 1);
			}
		}
		
		return MathUtils.capAngle( screen.projectAngle( direction.computeAngle() ) );
	}
	
	private static Vector2 computeCentroid( CvSeq contour ) {
		CvMoments m = new CvMoments();
		cvMoments( contour, m, 0 );
		
		return new Vector2( m.m10() / m.m00(), m.m01() / m.m00() );
	}
	
	private void drawCenteredRectangle( IplImage image, Vector2 p, int size, CvScalar colour, int thickness ) {
		Vector2 szv = new Vector2(size/2.0,size/2.0);
		CvPoint p1 = p.minus( szv ).getCvPoint();
		CvPoint p2 = p.plus( szv ).getCvPoint();
		cvRectangle( image, p1, p2, colour, thickness, 8, 0);
	}

	private double lineIntersection( Vector2 o1, Vector2 d1, Vector2 o2, Vector2 d2 ) {
		return ( o2.minus(o1) ).cross(d2) / d1.cross(d2);
	}
	
	private CvSeq getContours( IplImage img, IplImage temp ) {
		CvSeq seq = new CvSeq(null);
		cvCopy( img, temp );
		cvFindContours( temp, storage, seq, Loader.sizeof(CvContour.class), CV_RETR_LIST, CV_CHAIN_APPROX_SIMPLE );
		return seq;
	}

	private void loadCalibrationInfo() {
		CvMat intrinsics = new CvMat(cvLoad(dataFolder + "/Intrinsics.yml"));
		CvMat distortion = new CvMat(cvLoad(dataFolder + "/Distortion.yml"));
		
		if( intrinsics == null || distortion == null ) {
			System.err.println("Can't open distortion info.");
		} else {
			mapx = IplImage.create(cvGetSize(raw), IPL_DEPTH_32F, 1);
			mapy = IplImage.create(cvGetSize(raw), IPL_DEPTH_32F, 1);
			cvInitUndistortMap(intrinsics, distortion, mapx, mapy);
		}
	}
	
	private void undistort() {
		if( mapx == null || mapy == null ) {
			undistorted = raw;
		} else {
			cvCopy( raw, uncropped3[0] );
			cvRemap( uncropped3[0], undistorted, mapx, mapy, CV_INTER_LINEAR | CV_WARP_FILL_OUTLIERS, cvScalarAll(0) );
			//cvCopy( raw, undistorted );
		}
	}
	
	private void applyCrop() {
		clampCropRect( cropRect, undistorted );
		cvSetImageROI( undistorted, cropRect );
		cvCopy( undistorted, cropped );
		cvResetImageROI( undistorted );
	}
	
	private void allocUncroppedImages( BufferedImage frame ) {
		raw         = IplImage.createFrom( frame );
		undistorted = newImage( raw, 3 );
		
		uncropped3 = new IplImage[ UNCROPPED_THREE_CHANNEL_IMAGES ];
		for( int i = 0 ; i < uncropped3.length ; ++ i )
			uncropped3[ i ] = newImage( raw, 3 );
		
		uncropped1 = new IplImage[ UNCROPPED_ONE_CHANNEL_IMAGES ];
		for( int i = 0 ; i < uncropped1.length ; ++ i )
			uncropped1[ i ] = newImage( raw, 1 );
	}
	
	private void allocCroppedImages() {
		assert raw != null;
		
		cropped = IplImage.create( cvSize(cropRect.width(), cropRect.height()), raw.depth(), 3 );
		
		cropped1 = new IplImage[  CROPPED_ONE_CHANNEL_IMAGES  ];
		cropped3 = new IplImage[ CROPPED_THREE_CHANNEL_IMAGES ];
		
		for( int i = 0 ; i < cropped1.length ; ++ i )
			cropped1[ i ] = newImage( cropped, 1 );
		
		for( int i = 0 ; i < cropped3.length ; ++ i )
			cropped3[ i ] = newImage( cropped, 3 );
	}
	
	private static IplImage newImage( IplImage img, int channels ) {
		return IplImage.create( cvGetSize(img), img.depth(), channels );
	}
	
	private static void clampCropRect( CvRect crop, IplImage img ) {
		if( crop.x() < 0 ) crop.x( 0 );
		if( crop.y() < 0 ) crop.y( 0 );
		
		if( crop.width()  + crop.x() > img.width() )
			crop.width( img.width() - crop.x() );
		
		if( crop.height() + crop.y() > img.height() )
			crop.height( img.height() - crop.y() );
	}
	
	private static int clamp255( int x ) {
		if( x < 0 )
			return 0;
		else if( x > 255 )
			return 255;
		else
			return x;
	}
	
	private boolean isTraining() {
		return frameId < trainUntil;
	}
	
	private boolean isJustDoneTraining() {
		return frameId == trainUntil;
	}
	
	private class EntityLearner {
		private double   sum0 = 0;
		private double[] sum1 = { 0, 0, 0 };
		private double[] sum2 = { 0, 0, 0 };
		private Entity entity;
		
		public EntityLearner( Entity e ) {
			entity = e;
		}
		
		public void learn( CvRect rect ) {
			clampCropRect( rect, cropped );
			int newCount = rect.width()*rect.height();
			
			if( newCount == 0 )
				return;
			
			// udpate sum0: sum(x^0)
			sum0 += newCount;
			
			IplImage hsv = cropped3[0], blur = cropped3[1];
			IplImage tmp = IplImage.create(cvSize(rect.width(),rect.height()), IPL_DEPTH_64F, 3);
			
			cvSmooth(cropped, blur, CV_BLUR, 3);
			cvCvtColor( blur, hsv, CV_BGR2HSV );
			cvSetImageROI( hsv, rect );
			cvConvertScale( hsv, tmp, 1.0/255.0, 0);
			cvResetImageROI( hsv );
			
			// update sum1: sum(x^1)
			CvScalar scalar = cvSum( tmp );
			sum1[ 0 ] += scalar.getVal(0);
			sum1[ 1 ] += scalar.getVal(1);
			sum1[ 2 ] += scalar.getVal(2);
			
			// update sum2: sum(x^2)
			cvMul(tmp, tmp, tmp, 1.0);
			scalar = cvSum( tmp );
			sum2[ 0 ] += scalar.getVal(0);
			sum2[ 1 ] += scalar.getVal(1);
			sum2[ 2 ] += scalar.getVal(2);
			
			refresh();
		}
		
		public void refresh() {
			if( sum0 > 0 ) {
				// Update mean & std. dev
				setProperty( entity, Property.HUE,        Value.MEAN,  getMean( 0 ) );
				setProperty( entity, Property.HUE,        Value.RANGE, getRange( 0 ) );
				setProperty( entity, Property.VALUE,      Value.MEAN,  getMean( 1 ) );
				setProperty( entity, Property.VALUE,      Value.RANGE, getRange( 1 ) );
				setProperty( entity, Property.SATURATION, Value.MEAN,  getMean( 2 ) );
				setProperty( entity, Property.SATURATION, Value.RANGE, getRange( 2 ) );
			}
		}
		
		public int getMean( int i ) {
			return (int) Math.round((sum1[ i ] / sum0) * 255.0);
		}
		
		public double getStd( int i ) {
			return Math.sqrt( sum0 * sum2[ i ] - sum1[ i ] * sum1[ i ] ) / sum0 * 255.0 * (i>0?4.0:1.0);
		}
		
		public int getRange( int i ) {
			return (int) Math.round( getStd( i ) * kSigma );
		}
	}
}
